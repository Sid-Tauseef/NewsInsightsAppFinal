# import spacy
# import requests
# import pandas as pd
# import numpy as np
# import re
# from sentence_transformers import SentenceTransformer
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# from nltk.corpus import stopwords
# from collections import Counter, defaultdict
# import logging
# from langdetect import detect, LangDetectException
# from datetime import datetime

# logger = logging.getLogger(__name__)

# class RecommendationService:
#     def __init__(self):
#         self.NEWSAPI_KEY = "f920a5d9981e42de91c052c8471db7a2"
#         self.NEWSAPI_URL = "https://newsapi.org/v2/everything"
#         self.nlp = spacy.load("en_core_web_sm")
#         self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
#         self.tfidf_vectorizer = TfidfVectorizer(stop_words='english')
#         self.ENGLISH_STOP_WORDS = set(stopwords.words('english'))
#         self.article_cache = defaultdict(list)

#     def detect_language(self, text):
#         try:
#             return detect(text)
#         except LangDetectException:
#             return None

#     def _preprocess_text(self, text):
#         text = re.sub(r"[^\w\s]", "", text.lower())
#         doc = self.nlp(text)
#         return " ".join([
#             token.lemma_ for token in doc
#             if token.is_alpha and token.text not in self.ENGLISH_STOP_WORDS
#         ])

#     def _extract_keywords(self, text, top_n=5):
#         doc = self.nlp(text.lower())
#         keywords = [
#             token.text for token in doc
#             if token.is_alpha and token.text not in self.ENGLISH_STOP_WORDS
#             and token.pos_ in ["NOUN", "PROPN"]
#         ]
#         keyword_counts = Counter(keywords)
#         top_keywords = [word for word, _ in keyword_counts.most_common(top_n)]
#         return " ".join(top_keywords) if top_keywords else text.split()[0]

#     def _fetch_news(self, query):
#         params = {
#             "q": query,
#             "apiKey": self.NEWSAPI_KEY,
#             "language": "en",
#             "pageSize": 50,
#             "sortBy": "relevancy"
#         }
#         try:
#             response = requests.get(self.NEWSAPI_URL, params=params)
#             response.raise_for_status()
#             data = response.json()
#             return data.get("articles", [])
#         except Exception as e:
#             logger.error(f"News API error: {str(e)}")
#             return []

#     def _process_article(self, article):
#         return {
#             "title": article.get("title", "No Title"),
#             "description": article.get("description", ""),
#             "url": article.get("url", "#"),
#             "urlToImage": article.get("urlToImage", ""),
#             "source": {"name": article.get("source", {}).get("name", "Unknown")},
#             "publishedAt": article.get("publishedAt", ""),
#             "processed_text": ""
#         }

#     def _calculate_semantic_similarity(self, text1, text2):
#         emb1 = self.sbert_model.encode([self._preprocess_text(text1)])[0]
#         emb2 = self.sbert_model.encode([self._preprocess_text(text2)])[0]
#         return cosine_similarity([emb1], [emb2])[0][0]

#     def _calculate_keyword_overlap(self, text1, text2):
#         keywords1 = set(self._extract_keywords(text1).split())
#         keywords2 = set(self._extract_keywords(text2).split())
#         if not keywords1 and not keywords2:
#             return 1.0
#         intersection = len(keywords1 & keywords2)
#         union = len(keywords1 | keywords2)
#         return intersection / union if union != 0 else 0.0

#     def evaluate_recommendations(self, liked_article, recommendations):
#         if not recommendations:
#             return {}
        
#         base_title = liked_article["title"]
#         base_keywords = self._extract_keywords(base_title)
        
#         metrics = {
#             "semantic_similarity": [],
#             "keyword_overlap": [],
#             "category_match": [],
#             "relevance_score": 0
#         }
        
#         for rec in recommendations:
#             if not rec.get("title"):
#                 continue  # Skip invalid articles
            
#             try:
#                 semantic_score = self._calculate_semantic_similarity(base_title, rec["title"])
#             except Exception:
#                 semantic_score = 0.0
#             metrics["semantic_similarity"].append(semantic_score)
            
#             try:
#                 keyword_score = self._calculate_keyword_overlap(base_title, rec["title"])
#             except Exception:
#                 keyword_score = 0.0
#             metrics["keyword_overlap"].append(keyword_score)
            
#             category_match = int(
#                 rec.get("category", "general") == 
#                 liked_article.get("category", "general")
#             )
#             metrics["category_match"].append(category_match)
        
#         avg_semantic = np.mean(metrics["semantic_similarity"]) if metrics["semantic_similarity"] else 0.0
#         avg_keyword = np.mean(metrics["keyword_overlap"]) if metrics["keyword_overlap"] else 0.0
#         category_acc = np.mean(metrics["category_match"]) if metrics["category_match"] else 0.0
        
#         metrics["avg_semantic"] = float(avg_semantic)
#         metrics["avg_keyword"] = float(avg_keyword)
#         metrics["category_accuracy"] = float(category_acc)
        
#         metrics["relevance_score"] = (
#             0.6 * avg_semantic + 
#             0.3 * avg_keyword + 
#             0.1 * category_acc
#         )
        
#         return metrics

#     def _get_article_recommendations(self, liked_article):
#         title = liked_article.get("title", "")
#         if not title or self.detect_language(title) != "en":
#             return []
        
#         keywords = self._extract_keywords(title)
#         raw_articles = self._fetch_news(keywords)
#         if not raw_articles:
#             return []
        
#         articles_df = pd.DataFrame([self._process_article(a) for a in raw_articles])
#         articles_df["processed_text"] = articles_df["title"].apply(self._preprocess_text)
        
#         if articles_df.empty:
#             return []
        
#         processed_input = self._preprocess_text(title)
#         embeddings = self.sbert_model.encode(articles_df["processed_text"].tolist())
#         tfidf_matrix = self.tfidf_vectorizer.fit_transform(articles_df["processed_text"])
        
#         input_embedding = self.sbert_model.encode([processed_input])
#         sbert_scores = cosine_similarity(input_embedding, embeddings).flatten().astype(float)
#         tfidf_scores = cosine_similarity(
#             self.tfidf_vectorizer.transform([processed_input]),
#             tfidf_matrix
#         ).flatten().astype(float)
        
#         articles_df["score"] = 0.6 * sbert_scores + 0.4 * tfidf_scores
        
#         return articles_df.sort_values("score", ascending=False).head(3).to_dict("records")

#     def get_recommendations(self, liked_articles):
#         try:
#             if not liked_articles:
#                 return []
            
#             all_recommendations = []
#             seen_articles = set()
            
#             for article in liked_articles:
#                 if not isinstance(article, dict) or "title" not in article:
#                     logger.error(f"Invalid article format: {article}")
#                     continue
                
#                 recs = self._get_article_recommendations(article)
#                 for rec in recs:
#                     if rec["title"] not in seen_articles:
#                         all_recommendations.append(rec)
#                         seen_articles.add(rec["title"])
            
#             def sort_key(article):
#                 date_score = 0
#                 try:
#                     days_old = (datetime.now() - datetime.fromisoformat(article["publishedAt"][:-1])).days
#                     date_score = max(0, 1 - (days_old / 30))
#                 except:
#                     pass
#                 score = float(article.get("score", 0.0))
#                 return score + date_score
            
#             sorted_recs = sorted(all_recommendations, key=sort_key, reverse=True)[:10]
#             for rec in sorted_recs:
#                 rec.pop("score", None)
            
#             if sorted_recs and liked_articles:
#                 evaluation = self.evaluate_recommendations(liked_articles[0], sorted_recs)
#                 logger.info(
#                     f"Recommendation Quality Metrics:\n"
#                     f"Avg Semantic Similarity: {evaluation['avg_semantic']:.2f}\n"
#                     f"Avg Keyword Overlap: {evaluation['avg_keyword']:.2f}\n"
#                     f"Category Accuracy: {evaluation['category_accuracy']:.2f}\n"
#                     f"Combined Relevance Score: {evaluation['relevance_score']:.2f}"
#                 )
            
#             return sorted_recs
#         except Exception as e:
#             logger.error(f"Recommendation error: {str(e)}")
#             return []